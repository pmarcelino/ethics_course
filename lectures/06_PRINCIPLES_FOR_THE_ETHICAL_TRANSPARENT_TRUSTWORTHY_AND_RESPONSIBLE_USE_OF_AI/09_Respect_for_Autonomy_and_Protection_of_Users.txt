Hi everyone, and welcome to our final session in this module. We’ve covered a lot of ground so far, and last time we dug into the nuts and bolts of legality and making sure digital systems stay on the right side of the law. Today, we’re going to focus on something just as critical, if not more so: how we actually respect people’s autonomy and make sure users are truly protected in the world of AI-driven mobility.

Let’s kick things off by talking a bit about autonomy. If you think about it, autonomy is really about giving users the steering wheel—not just literally, but figuratively too. Imagine you’re using an e-scooter app. You expect to see a map, choose your own route, decide when to start and stop, and get clear info about pricing. It wouldn’t feel right if the app quietly nudged you toward longer, more expensive trips just because it thinks you’re willing to pay more. That’s not real choice—that’s manipulation. So, the challenge for designers is to create systems that inform and empower, rather than steer people in directions they haven’t chosen.

Now, how do we make sure users actually have that control? Well, one big piece of the puzzle is transparency. If an app is using AI to suggest a certain route or service, users should know about it and, ideally, get a simple explanation for why that suggestion is popping up. Maybe the AI recommends a detour because of traffic or safety concerns. If users can see the reasoning, they’re better equipped to make their own decisions. Think of it like having a helpful friend who tells you not just what to do, but why they’re suggesting it.

But, as much as we want to give users freedom, we also have to provide a safety net. Protecting users from unfair treatment is non-negotiable. Just picture two passengers being quoted wildly different prices for the same journey, simply because of what an algorithm guesses about their spending habits. Not only does that feel wrong, but it’s also banned under laws designed to protect consumers. So fairness and equal treatment have to be baked into these systems from the start.

Of course, none of this works if users can’t trust platforms with their personal data. Mobility apps know where you go, when you travel, maybe even who you travel with. That’s sensitive information, and people deserve to know what’s being collected, how it’s handled, and what rights they have—like correcting mistakes or deleting their data altogether. Clear privacy protections aren’t just a nice-to-have; they’re required by laws like the GDPR and are essential for keeping users’ trust.

And let’s not forget access to rights and remedies. AI should make it easier—not harder—for people to know what they’re entitled to, whether it’s a refund after a ride goes wrong or understanding how to cancel a booking. The best systems put this information front and center, so users always know where they stand.

So, to bring it all together: respecting autonomy and protecting users aren’t just lofty goals. They’re concrete requirements grounded in law and essential for earning public trust. If we get this right, we don’t just create smarter mobility services—we build systems people actually want to use. Thanks for joining me throughout this module.