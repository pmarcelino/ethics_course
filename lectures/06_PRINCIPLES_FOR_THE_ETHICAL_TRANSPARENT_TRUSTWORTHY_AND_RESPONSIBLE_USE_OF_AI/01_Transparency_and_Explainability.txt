Hey everyone, and welcome to our module on responsible AI in urban mobility and transport markets. Today, we’re starting with two essentials for making AI work well in our cities: transparency and explainability. Since this is our first topic, let’s look at why they matter and what they mean in practice.

Imagine using a bike-sharing app, and suddenly your usual route costs twice as much. Or you’re on a train that unexpectedly skips your stop because a “smart” routing system decided it was more efficient. If you don’t know why, it’s frustrating—and that’s where transparency and explainability come in.

Transparency is like shining a flashlight into the engine of a self-driving car. It’s about showing how an AI system takes in information, processes it, and makes decisions. This means clear documentation, open processes, and a willingness to “show your work.” It’s the difference between being handed a mystery dish and having the chef walk you through the recipe.

Explainability is related but distinct. If transparency opens the kitchen, explainability makes sure the recipe is understandable to everyone—not just engineers. Explanations must be clear, jargon-free, and relevant to whoever’s affected, whether that’s city officials, commuters, or technicians.

Some systems are easier to explain than others. Traditional, rule-based programs—like a flowchart of boxes and arrows—are straightforward. You can see the rules and logic. But with machine learning, especially complex models, it’s trickier. It’s like trying to understand a chess grandmaster’s moves: you know they’re smart, but without an explanation, you can’t follow the reasoning.

So, how do we make transparency and explainability real? Before deploying an AI system, define its purpose, data use, and known limitations. Once it’s operating, keep decision logs and allow audits to ensure accountability if things go wrong.

In urban mobility—whether it’s bus routing, traffic light control, or fare calculation—these principles help commuters understand and question results, and help city leaders prove systems are fair and safe. They build trust, reduce misunderstandings, and ensure that automation serves everyone, not just a few.

Ultimately, transparency and explainability set the stage for good governance in AI-powered transport. They make it possible to hold systems accountable, adapt when mistakes happen, and ensure benefits are shared widely. As we move forward, our next topic will explore accountability—how to make sure those building and running AI systems take responsibility for their impact. Thanks for joining me for this opening session. I’m looking forward to digging deeper with you next time.