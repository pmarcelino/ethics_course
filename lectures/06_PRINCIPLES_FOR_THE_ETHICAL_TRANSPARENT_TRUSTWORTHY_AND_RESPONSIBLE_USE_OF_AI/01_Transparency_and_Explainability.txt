Hi everyone, and welcome to the start of our module on responsible AI in urban mobility and transport markets. Today, we’re diving into two of the key ingredients for making artificial intelligence work well in our cities: transparency and explainability. Since this is our very first topic, let’s take a little time to unpack why they matter and what they actually mean in practice.

Let’s start with this: imagine you’re using a bike-sharing app, and all of a sudden, the price for your usual route doubles. Or say you’re on a train, and it unexpectedly skips your stop because some “smart” routing system decided it was more efficient. If you have no idea why these things happen, it’s bound to be frustrating—and that’s exactly where transparency and explainability come in.

So, what do we really mean by transparency? Picture it as shining a flashlight into the engine of a self-driving car. Transparency is about making sure we can trace how an AI system takes in information, processes it, and spits out a decision. It involves clear documentation, open processes, and a willingness to show your work. In a sense, it’s the difference between a chef handing you a mystery dish and one who walks you through the recipe, step by step.

Now, explainability is a close cousin, but it’s got its own flavor. If transparency is about opening the kitchen, explainability is about describing the recipe so anyone at the table can understand what’s on their plate. It’s not enough for AI systems to be transparent only to engineers. The explanations need to be clear, jargon-free, and meaningful to everyone involved—whether that's city officials, transit riders, or even the person fixing a broken bus stop sensor.

Of course, some systems are easier to explain than others. The older, rule-based programs—think of them like a flowchart with boxes and arrows—are fairly straightforward. You can see the rules and follow the logic. But as soon as you introduce machine learning, especially the more complex models, things get fuzzy. It’s a little like trying to read the mind of a chess grandmaster: you know they’re making smart moves, but it’s hard to say exactly why unless they walk you through their thinking.

So, how do we put transparency and explainability into action? Before rolling out an AI system, it’s crucial to spell out what it’s supposed to do, how it will use data, and where its blind spots might be. Once it’s up and running, keeping a record of decisions and making it possible to audit those choices allows for accountability if something goes wrong.

When we talk about urban mobility—be it bus routing, traffic signal control, or fare calculation—these principles help everyone stay on the same page. They let commuters understand and challenge outcomes, and they help city leaders show that systems are being run fairly and safely.

Ultimately, transparency and explainability are what set the stage for trust and good governance in AI-powered transport. And as we move forward, we’ll see how these ideas connect to the next major theme: accountability. Thanks for joining me for this opening session—I’m looking forward to digging deeper with you next time.