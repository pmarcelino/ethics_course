Welcome back, everyone. Today, we’re tackling two important topics: identifying risks and ensuring data quality. 

When AI is deployed in city transport—whether managing shared scooters or predicting congestion—it’s tempting to focus only on the benefits. But our first responsibility is to ask: what could go wrong? 

Start with fundamental rights risks. If your system learns from biased historical data—say, fewer buses in low-income neighborhoods—it can reinforce inequities. That’s like baking a cake with spoiled ingredients: no matter the recipe, the result is flawed.

Then there’s privacy. AI systems thrive on sensitive information—trip histories, GPS traces, payment records. If mishandled, that data can be exposed, undermining public trust.

Security is another major front. Systems controlling infrastructure like traffic lights are prime hacking targets. A cyberattack here could delay emergency vehicles or paralyze key routes. And don’t overlook transparency—when AI acts like a “black box,” it’s hard for leaders or citizens to challenge bad decisions.

To prioritize action, many teams use risk matrices. Think of them as a way to map out which risks are most likely and which could do the most damage, so effort goes where it matters most. But here’s the thing: each risk isn’t just a technical problem. There’s a financial side—like the cost of fixing a data breach or paying fines—but also intangible effects. How do you put a price on lost public trust, or on people deciding not to use a new transit service because they’re worried about privacy?

And many of these risks trace back to data quality. Think of data as your ingredients: stale or incomplete inputs make for unreliable outcomes.

Good data starts with representativeness. If your bus scheduling AI only learns from downtown routes, other areas will be underserved. Build datasets that reflect the full community—age, income, location—and revisit them regularly to catch gaps.

Of course, even with the best intentions, bias can sneak in. How do we catch it? Well, modern AI projects use a mix of automated tools and good old-fashioned human oversight. Use alerts to flag anomalies—like certain neighborhoods consistently receiving fewer transit options—and follow up with periodic audits. When issues arise, fix them quickly by adjusting sources or retraining the system.

Data integrity matters too. From collection to analysis, information must stay accurate and secure. Imagine tampered traffic data sending vehicles into gridlock—that’s why encryption, access controls, and backups are non-negotiable. Automation can help here, scanning for missing or outdated information and flagging problems for human review.

Ultimately, risk identification and data quality are two sides of the same coin. Spotting threats is only half the job; making sure your data is fair, accurate, and secure gives your AI a fighting chance to avoid them in the first place.

So, as we look ahead to our next topic—equity and accessibility—keep in mind that the path to fair and effective AI starts with getting the data right. See you next time!
