Hey everyone, welcome back! Today, we’re tackling a topic that’s easy to oversimplify but actually has more layers than most people realize: compliance that goes beyond the EU’s AI Regulation.

And while we’re using Europe as our example today, the same principle holds elsewhere: in the US, Canada, or Singapore, AI-specific rules also need to be read alongside data protection, sector-specific, and cybersecurity laws. Different recipe, same layered cake.

So, let’s set the scene in Europe. Imagine you’re running a smart mobility service in Lisbon—an app predicting bus arrivals, or a platform matching riders with scooters. The AI Regulation is your foundation: a single, unified set of rules across the EU, ensuring safe, transparent, and accountable AI. It’s like replacing a jumble of local road signs with one universal traffic signal system.

But that’s only the first slice of the cake. The moment your system touches personal data—route preferences, payment info, location history—you also need to follow the GDPR, Europe’s gold standard for data protection. That means having a lawful basis for processing data, minimizing what you collect, anonymizing where possible, and respecting user rights to access, correct, or delete their data. Think of it as having a customer service desk—just for privacy.

Next come sector-specific rules. In mobility, the ITS Directive sets the ground rules for intelligent transport systems, while national laws—say, in Portugal—outline operational requirements. EU-level transport regulations aim to ensure fair competition and user protection, reminding us that compliance extends beyond your app’s code to the wider transport ecosystem. Many other jurisdictions have their own sectoral frameworks—sometimes stricter, sometimes looser, but the idea is the same: general AI rules are only one layer.

We can’t forget other elements: the Cybersecurity Act for securing networks, the Digital Services Act for platform responsibilities, and similar frameworks worldwide that keep digital infrastructure resilient and trustworthy.

Now, here’s where the AI Regulation’s risk-based approach comes into play. Not all AI is regulated equally—it depends on the likelihood and severity of potential harm. The framework defines four categories:
Unacceptable risk – which is banned outright (like manipulative AI targeting vulnerable groups).
High risk – under strict oversight (like traffic management AI influencing safety-critical decisions).
Limited risk – which has transparency requirements (like telling users they’re talking to a chatbot).
Minimal risk – requiring no regulation (for example, AI recommending bus route maps).

This context-driven model means a bus schedule chatbot won’t face the same scrutiny as an autonomous vehicle. Other countries use different terminology, but many follow a similar logic—matching rules to potential harm.

In short: compliance in AI-powered mobility is a blend of AI-specific rules, data protection laws, sector regulations, and security obligations—all filtered through a risk-based lens. Whether in Europe or elsewhere, getting this right isn’t just about avoiding penalties—it’s about building systems that people can trust.

Next up, we’ll zero in on what makes mobility and transport such a hot spot for high-risk AI. See you then!
